{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "# import for time, and file change\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# frequent updating\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filepaths\n",
    "workspace = \"/Users/advaysingh/Documents/projects/hindi_classification/\" \n",
    "#print(\"Current workspace:\", workspace)\n",
    "\n",
    "data = os.path.join(workspace, 'data/Hindi/')\n",
    "dict_lib = os.path.join(workspace, 'data/dict.csv')\n",
    "img_path = os.path.join(workspace, 'server/snapshot.png')\n",
    "out_file = os.path.join(workspace, 'server/outputs.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/test dicts with files and labels\n",
    "\n",
    "def create_dict(x: str) -> dict:\n",
    "    x_dict = {}\n",
    "    i = 0\n",
    "    for dir in os.listdir(os.path.join(data, x)):\n",
    "        for file in os.listdir(os.path.join(data, x, dir)):\n",
    "            x_dict[os.path.join(data, x, dir, file)] = i\n",
    "        i += 1\n",
    "    return x_dict\n",
    "\n",
    "# make pandas df\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(create_dict('Train'), orient='index')\n",
    "test_df = pd.DataFrame.from_dict(create_dict('Test'), orient='index')\n",
    "\n",
    "#print(train_df)\n",
    "#df_temp = train_df[0].drop_duplicates()\n",
    "# print(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057317038046, 0.24472847105479767, 0.6652409557748219]\n"
     ]
    }
   ],
   "source": [
    "# Activations class\n",
    "class Activation:\n",
    "    # can add more types\n",
    "    def __init__(self, act_type: str) -> None:\n",
    "        self.act = act_type\n",
    "\n",
    "    def print_act(self) -> None:\n",
    "        return self.act\n",
    "    \n",
    "    def compute(self, z) -> list:\n",
    "\n",
    "        # Sigmoid function\n",
    "        if (self.act == 'sigmoid'):\n",
    "            n = []\n",
    "            for val in (z):\n",
    "                n.append(1.0 / (1.0 + np.exp(val)))\n",
    "            return n\n",
    "        \n",
    "        # ReLU function\n",
    "        elif (self.act == 'relu'):\n",
    "            vals = []\n",
    "            for val in z:\n",
    "                if val < 0.0:\n",
    "                    vals.append(0.0)\n",
    "                else:\n",
    "                    vals.append(1.0)\n",
    "            return vals\n",
    "\n",
    "    def prime(self, z: list):\n",
    "\n",
    "        # Sigmoid prime\n",
    "        if (self.act == 'sigmoid'):\n",
    "            sigs = self.compute(z)\n",
    "            vals = []\n",
    "            for val in sigs:\n",
    "                vals.append(val * (1 - val))\n",
    "            return vals\n",
    "        \n",
    "        # ReLU prime\n",
    "        if (self.act == 'relu'):\n",
    "            vals = []\n",
    "            for val in z:\n",
    "                if val == 0:\n",
    "                    vals.append(0.0)\n",
    "                else:\n",
    "                    vals.append(1.0)\n",
    "            return vals\n",
    "\n",
    "\n",
    "def softmax(costs: list) -> list:\n",
    "    exp_vals = []\n",
    "    for cost in np.array(costs):\n",
    "        exp_vals.append(np.exp(cost))\n",
    "    return_vals = []\n",
    "    for i in range(len(exp_vals)):\n",
    "        return_vals.append(exp_vals[i] / sum(exp_vals))\n",
    "    return return_vals\n",
    "\n",
    "def softmax_prime(costs: list) -> list:\n",
    "    softs = softmax(costs)\n",
    "    vals = []\n",
    "    for val in softs:\n",
    "        vals.append(val * (1 - val))\n",
    "    return vals\n",
    "\n",
    "n = softmax([2, 3, 4])\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"class Model:\n",
    "    def __init__(self, train_df: pd.DataFrame,\n",
    "                  act: Activation, layers: list, step: float, stochastic: bool, load_perams: bool) -> None:\n",
    "        self.train_df = train_df\n",
    "\n",
    "        # create activation function\n",
    "        self.act = act\n",
    "        self.layers = layers\n",
    "\n",
    "        # init weight and bias with # layers np arrays\n",
    "        # Using the Xavier weight init method https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "        self.all_weights = [None] * (int(len(layers)))\n",
    "        self.all_bias = [None] * (int(len(layers)))\n",
    "        if (load_perams):\n",
    "            layer = 0\n",
    "            weightlist = os.listdir(os.path.join(workspace, 'data', 'Hyper_p/Weights'))\n",
    "            for file in sorted(weightlist, key=lambda s: s.lower()):\n",
    "                self.all_weights[layer] = pd.read_csv(os.path.join(workspace, 'data', 'Hyper_p/Weights', file)).to_numpy()\n",
    "                #self.all_weights[layer] = np.delete(self.all_weights, [0], axis=1)\n",
    "                print(self.all_weights[layer].shape)\n",
    "                layer += 1\n",
    "            layer = 0\n",
    "            biaslist = os.listdir(os.path.join(workspace, 'data', 'Hyper_p/Biases'))\n",
    "            for file in sorted(biaslist, key=lambda s: s.lower()):\n",
    "                self.all_bias[layer] = pd.read_csv(os.path.join(workspace, 'data', 'Hyper_p/Biases', file)).to_numpy()\n",
    "                layer += 1\n",
    "            print(self.all_weights[0])\n",
    "        else:\n",
    "            for i in range(len(layers)):\n",
    "                self.all_weights[i] = self.random_arrs(i)\n",
    "                if i == 0:\n",
    "                    self.all_bias[i] = np.random.uniform(low=-0.03125, high=0.03125, size=(self.layers[i],1))\n",
    "                else:\n",
    "                    bias_val = 1 / math.sqrt(self.layers[i - 1])\n",
    "                    self.all_bias[i] = np.random.uniform(low=-bias_val, high=bias_val, size=(self.layers[i],1))\n",
    "            #for val in self.all_weights:\n",
    "               # print(val.shape)\n",
    "\n",
    "        # init learn speed\n",
    "        self.speed = step\n",
    "\n",
    "        # determine gradient decent type\n",
    "        self.stochastic = stochastic\n",
    "\n",
    "    def random_arrs(self, layer: int):\n",
    "        if (layer == 0):\n",
    "            return(np.random.uniform(low=-0.03125, high=0.03125, size=(self.layers[layer],1024)))\n",
    "        else:\n",
    "            weight_val = 1 / math.sqrt(self.layers[layer - 1])\n",
    "            return(np.random.uniform(low=-weight_val, high=weight_val, size=(self.layers[layer], self.layers[layer-1])))\n",
    "\n",
    "    def img_to_np(self, dir) -> np.array:\n",
    "        return np.array(Image.open(dir)).flatten()\n",
    "\n",
    "\n",
    "    def print_weights(self) -> None:\n",
    "        for arr in self.all_weights:\n",
    "            print(arr.shape)\n",
    "\n",
    "    def prop_forward(self, inputs: list, layer: int) -> list:\n",
    "        a_vals = [0.0] * int(self.layers[layer])\n",
    "        a_vals = np.dot(self.all_weights[layer], pd.DataFrame(inputs)) + self.all_bias[layer]\n",
    "        if (layer == len(self.layers) - 1):\n",
    "            return (softmax(a_vals))\n",
    "        return(self.act.compute(a_vals))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def prop_backword(self, x: list, label: list):\n",
    "        delta_w = [None] * len(self.layers)\n",
    "        delta_b = [None] * len(self.layers)\n",
    "        inputs = []\n",
    "        for i in range(len(self.layers)):\n",
    "            inputs.append(x)\n",
    "            x = self.prop_forward(x, i)\n",
    "        error = [a - b for a, b in zip(x, label)]\n",
    "        vals = np.array([a * b for a, b in zip(error, softmax_prime(x))])\n",
    "        for i in range(len(self.layers), 0, -1):\n",
    "            delta_w[i - 1] = np.dot(pd.DataFrame(vals), pd.DataFrame(inputs[i - 1]).transpose())\n",
    "            delta_b[i - 1] = vals\n",
    "            vals = np.dot(pd.DataFrame(self.all_weights[i - 1]).transpose(), pd.DataFrame(vals))\n",
    "            vals = [a * b for a, b in zip(vals, self.act.prime(inputs[i - 1]))]\n",
    "            #print(np.dot(pd.DataFrame(vals), pd.DataFrame(inputs[0]).transpose()))\n",
    "        #print(delta_w[0])\n",
    "        return delta_w, delta_b\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    # potentially reconsider for time complexity\n",
    "    def vals_for_softmax(self, index: int) -> list:\n",
    "        vals = self.train_df[0].unique()\n",
    "        for i in range(len(vals)):\n",
    "            if vals[i] != index:\n",
    "                vals[i] = 0\n",
    "            else:\n",
    "                vals[i] = 1\n",
    "        return vals\n",
    "\n",
    "\n",
    "    def train(self, epochs: int) -> None:\n",
    "        length_set = len(self.train_df)\n",
    "        for epoch in  range(epochs):\n",
    "            delta_w = [np.zeros_like(array) for array in self.all_weights]\n",
    "            delta_b = [np.zeros_like(array) for array in self.all_bias]\n",
    "            count = 0\n",
    "            for img_dir, row in self.train_df.iterrows():\n",
    "                print(\"img\", count)\n",
    "                count += 1\n",
    "                temp_w, temp_b = self.prop_backword(self.img_to_np(img_dir).tolist(), self.vals_for_softmax(row[0]))\n",
    "                for i in range(len(self.layers)):\n",
    "                    delta_b[i] += temp_b[i]\n",
    "                    delta_w[i] += temp_w[i]\n",
    "            delta_w = [array / length_set for array in delta_w]\n",
    "            delta_b = [np.array(array) / length_set for array in delta_b]\n",
    "            for i in range(len(self.layers)):\n",
    "                self.all_weights[i] += delta_w[i]\n",
    "                self.all_bias[i] += delta_b[i]\n",
    "            print(\"validating\")\n",
    "            print(\"epoch: \", epoch)\n",
    "            self.validate(self.train_df)\n",
    "\n",
    "    def pred(self, img: np.array) -> int:\n",
    "        for i in range(len(self.layers)):\n",
    "            img = self.prop_forward(img, i)\n",
    "        return img\n",
    "\n",
    "    def validate(self, test_df: pd.DataFrame) -> None:\n",
    "        count, countright = 0, 0\n",
    "        for img_dir, row in test_df.iterrows():\n",
    "            outputs = self.pred(self.img_to_np(img_dir))\n",
    "            actual = list(self.vals_for_softmax(row[0]))\n",
    "            if (outputs.index(max(outputs))) == (actual.index(max(actual))):\n",
    "                countright += 1\n",
    "            count += 1\n",
    "            accuracy = (float(countright) / float(count)) * 100\n",
    "        print(\"count: \", count, \"countright: \", countright, \". Accuracy: \", accuracy, \"%\")\n",
    "\n",
    "    def save(self) -> None:\n",
    "        hp_data = '/Users/advaysingh/Documents/projects/hindi_classification/data/hyper_p'\n",
    "        hp_data_biases, hp_data_weights = hp_data + '/Biases', hp_data + '/Weights'\n",
    "        os.makedirs(hp_data)\n",
    "        os.makedirs(hp_data_biases)\n",
    "        os.makedirs(hp_data_weights)\n",
    "        for layer in range(len(self.layers)):\n",
    "            df_weights = pd.DataFrame(self.all_weights[layer])\n",
    "            df_bias = pd.DataFrame(self.all_bias[layer])\n",
    "            weights_file_name = \"layer_\" + str(layer) + \"weights.csv\"\n",
    "            bias_file_name = \"layer_\" + str(layer) + \"bias.csv\"\n",
    "            weights_file = open(os.path.join(hp_data_weights, weights_file_name), \"x\")\n",
    "            bias_file = open(os.path.join(hp_data_biases, bias_file_name), \"x\")\n",
    "            df_weights.to_csv(weights_file, index=False)\n",
    "            df_bias.to_csv(bias_file, index=False)\n",
    "\n",
    "\n",
    "    def print_weights(self, layer: int) -> None:\n",
    "        for row in self.all_weights[layer]:\n",
    "            for weight in row:\n",
    "                print(weight)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(z: np.array) -> np.array:\n",
    "    \"\"\"Compute the sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def act_prime(z: np.array) -> np.array:\n",
    "    \"\"\"Compute the derivative of the sigmoid activation function.\"\"\"\n",
    "    ap = activate(z)\n",
    "    return ap * (1 - ap)\n",
    "\n",
    "def softmax(z: np.array) -> np.array:\n",
    "    e_z = np.exp(z - np.max(z)) # Improved stability\n",
    "    return e_z / e_z.sum(axis=0)\n",
    "\n",
    "def softmax_prime(z: np.array) -> np.array:\n",
    "    softmax_vals = softmax(z)\n",
    "    # For each i, j in softmax output vector size: [si * (1 - si)] if i == j, otherwise: -si * sj\n",
    "    return np.diag(softmax_vals) - np.outer(softmax_vals, softmax_vals)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, layers: list, load_perams: bool) -> None:\n",
    "        self.train_df = train_df\n",
    "        self.layers = layers\n",
    "\n",
    "        self.all_weights = [None] * len(self.layers)\n",
    "        self.all_bias = [None] * len(self.layers)\n",
    "\n",
    "        # read in weights if load perams\n",
    "        if (load_perams):\n",
    "            layer = 0\n",
    "            weightlist = os.listdir(os.path.join(workspace, 'data', 'Hyper_p/Weights'))\n",
    "            for file in sorted(weightlist, key=lambda s: s.lower()):\n",
    "                self.all_weights[layer] = (pd.read_csv(os.path.join(workspace, 'data', 'Hyper_p/Weights', file)).to_numpy())\n",
    "                layer += 1\n",
    "            layer = 0\n",
    "            biaslist = os.listdir(os.path.join(workspace, 'data', 'Hyper_p/Biases'))\n",
    "            for file in sorted(biaslist, key=lambda s: s.lower()):\n",
    "                self.all_bias[layer] = (pd.read_csv(os.path.join(workspace, 'data', 'Hyper_p/Biases', file)).to_numpy())\n",
    "                layer += 1\n",
    "\n",
    "        else:\n",
    "            for i in range(len(layers)):\n",
    "                self.all_weights[i] = self.random_arrs(i)\n",
    "                if i == 0:\n",
    "                    self.all_bias[i] = np.random.uniform(low=-0.03125, high=0.03125, size=(self.layers[i],1))\n",
    "                else:\n",
    "                    bias_val = 5 # TODO reconsider\n",
    "                    self.all_bias[i] = np.random.uniform(low=-bias_val, high=bias_val, size=(self.layers[i],1))\n",
    "    \n",
    "\n",
    "    def random_arrs(self, layer: int):\n",
    "        if (layer == 0):\n",
    "            return(np.random.uniform(low=-0.03125, high=0.03125, size=(self.layers[layer],1024)))\n",
    "        else:\n",
    "            weight_val = 1 / math.sqrt(self.layers[layer - 1])\n",
    "            return(np.random.uniform(low=-weight_val, high=weight_val, size=(self.layers[layer], self.layers[layer-1])))\n",
    "\n",
    "    def img_to_np(self, dir) -> np.array:\n",
    "        return np.array(Image.open(dir)).flatten()\n",
    "    \n",
    "    def prop_forward(self, x: np.ndarray):\n",
    "        activations = [x] #activations.size > zs\n",
    "        zs = []\n",
    "        for i in range(len(self.layers)):\n",
    "            #print(self.all_weights[i].shape)\n",
    "            x = np.matmul(self.all_weights[i], x) + self.all_bias[i]\n",
    "            zs.append(x)\n",
    "            if (i == len(self.layers) - 1):\n",
    "                x = softmax(x)\n",
    "            else:\n",
    "                x = activate(x)\n",
    "            activations.append(x)\n",
    "        return activations, zs\n",
    "\n",
    "\n",
    "    \n",
    "    def prop_backward(self, x: np.ndarray, y: np.ndarray):\n",
    "        delta_b = [np.zeros(b.shape) for b in self.all_bias]\n",
    "        delta_w = [np.zeros(w.shape) for w in self.all_weights]\n",
    "        activations, zs = self.prop_forward(x)\n",
    "        delta = activations[-1] - y #* softmax_prime(zs[-1]) #use zs delta\n",
    "        delta_b[-1] = delta\n",
    "        delta_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # loop through layers and now apply the sigmoid prime\n",
    "        for i in range(2, len(self.layers)):\n",
    "            sp = act_prime(zs[-i]) # No need to print here\n",
    "            delta = np.dot(self.all_weights[-i + 1].transpose(), delta) * sp \n",
    "            delta_b[-i] = delta\n",
    "            delta_w[-i] = np.dot(delta, activations[-i - 1].T)\n",
    "        return delta_w, delta_b\n",
    "    \n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=100, lr=1, verbose=True) -> None:\n",
    "            # Shuffle the data at the start of each epoch (optional)\n",
    "        e = 0\n",
    "        while (e < epochs):\n",
    "            rng = np.random.permutation(1024)\n",
    "            d_x = 1024\n",
    "            d_y = 46\n",
    "            for i in range(100):\n",
    "                x = X_train[rng[i]].reshape((d_x, 1))\n",
    "                y = y_train[rng[i]].reshape((d_y, 1))\n",
    "                delta_w, delta_b = self.prop_backward(x, y)\n",
    "                for i in range(len(self.all_weights)):\n",
    "                    self.all_weights[i] -= lr * delta_w[i]\n",
    "                    self.all_bias[i] -= lr * delta_b[i]\n",
    "\n",
    "            new_loss = self.calc_squared_loss(X_train, y_train)\n",
    "            if verbose:\n",
    "                print(f\"Epoch {e}, mean squared loss: {new_loss}\")\n",
    "            e+=1\n",
    "\n",
    "    def calc_squared_loss(self, X, y):\n",
    "        sum = 0\n",
    "        for x in range(len(X)):\n",
    "            y_v, _ = self.prop_forward(X[x].reshape(1024, 1))\n",
    "            sum += ((y[x].reshape(46, 1) - y_v[-1].reshape(46,1)) ** 2) / 2\n",
    "        return np.mean(sum)\n",
    "\n",
    "    def pred(self, img: np.array) -> int:\n",
    "        img, _ = self.prop_forward(img)\n",
    "        return img[-1]\n",
    "    \n",
    "    def validate(self, test_df: pd.DataFrame) -> None:\n",
    "        count, countright = 0, 0\n",
    "        for img_dir, row in test_df.iterrows():\n",
    "            outputs = self.pred(self.img_to_np(img_dir))\n",
    "            actual = self.vals_for_softmax(row[0])\n",
    "            if (outputs.index(max(outputs))) - (actual.index(max(actual))) < 3:\n",
    "                countright += 1\n",
    "            count += 1\n",
    "            accuracy = (float(countright) / float(count)) * 100\n",
    "        print(\"count: \", count, \"countright: \", countright, \". Accuracy: \", accuracy, \"%\")\n",
    "\n",
    "    def save(self) -> None:\n",
    "        hp_data = '/Users/advaysingh/Documents/projects/hindi_classification/data/hyper_p'\n",
    "        hp_data_biases, hp_data_weights = hp_data + '/Biases', hp_data + '/Weights'\n",
    "        os.makedirs(hp_data)\n",
    "        os.makedirs(hp_data_biases)\n",
    "        os.makedirs(hp_data_weights)\n",
    "        for layer in range(len(self.layers)):\n",
    "            df_weights = pd.DataFrame(self.all_weights[layer])\n",
    "            df_bias = pd.DataFrame(self.all_bias[layer])\n",
    "            weights_file_name = \"layer_\" + str(layer) + \"weights.csv\"\n",
    "            bias_file_name = \"layer_\" + str(layer) + \"bias.csv\"\n",
    "            weights_file = open(os.path.join(hp_data_weights, weights_file_name), \"x\")\n",
    "            bias_file = open(os.path.join(hp_data_biases, bias_file_name), \"x\")\n",
    "            df_weights.to_csv(weights_file, index=False)\n",
    "            df_bias.to_csv(bias_file, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vals_for_softmax(index: int) -> np.array:\n",
    "        \"\"\" Helper function to create softmax values\"\"\"\n",
    "        vals = np.zeros(46)\n",
    "        vals[index] = 1\n",
    "        vals.reshape(46, 1)\n",
    "        return vals\n",
    "test_df_sample = test_df.sample(500)\n",
    "\n",
    "\n",
    "X_train, y_train = [], []\n",
    "for img_dir, row in test_df.iterrows():\n",
    "            X_train.append(np.array(Image.open(img_dir)))\n",
    "            y_train.append(vals_for_softmax(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, mean squared loss: 157.08405534656308\n",
      "Epoch 1, mean squared loss: 156.1929755282482\n",
      "Epoch 2, mean squared loss: 159.77070189723207\n",
      "Epoch 3, mean squared loss: 163.04924958503312\n",
      "Epoch 4, mean squared loss: 168.4010344951296\n",
      "Epoch 5, mean squared loss: 167.80679348954877\n",
      "Epoch 6, mean squared loss: 169.96079835454665\n",
      "Epoch 7, mean squared loss: 173.223487336638\n",
      "Epoch 8, mean squared loss: 174.6706031850964\n",
      "Epoch 9, mean squared loss: 178.06019519210852\n",
      "Epoch 10, mean squared loss: 176.63696227615196\n",
      "Epoch 11, mean squared loss: 177.5241549002877\n",
      "Epoch 12, mean squared loss: 177.57866526155567\n",
      "Epoch 13, mean squared loss: 177.65901518321442\n",
      "Epoch 14, mean squared loss: 178.71589682225758\n",
      "Epoch 15, mean squared loss: 178.65090091081936\n",
      "Epoch 16, mean squared loss: 179.99028300698347\n",
      "Epoch 17, mean squared loss: 180.44331040078376\n",
      "Epoch 18, mean squared loss: 180.40896809393246\n",
      "Epoch 19, mean squared loss: 179.61990855611634\n",
      "Epoch 20, mean squared loss: 180.5491799982804\n",
      "Epoch 21, mean squared loss: 180.8511777172561\n",
      "Epoch 22, mean squared loss: 181.09892913247947\n",
      "Epoch 23, mean squared loss: 181.85757660847045\n",
      "Epoch 24, mean squared loss: 181.60931974561643\n",
      "Epoch 25, mean squared loss: 182.32761815142337\n",
      "Epoch 26, mean squared loss: 181.32984195249443\n",
      "Epoch 27, mean squared loss: 181.9599380349562\n",
      "Epoch 28, mean squared loss: 182.914982435516\n",
      "Epoch 29, mean squared loss: 183.13140984953185\n",
      "Epoch 30, mean squared loss: 182.62423591231308\n",
      "Epoch 31, mean squared loss: 181.78260400732455\n",
      "Epoch 32, mean squared loss: 182.69413324815733\n",
      "Epoch 33, mean squared loss: 184.23400955711156\n",
      "Epoch 34, mean squared loss: 183.2485602035478\n",
      "Epoch 35, mean squared loss: 184.66486012533136\n",
      "Epoch 36, mean squared loss: 183.99726933450617\n",
      "Epoch 37, mean squared loss: 183.4715596356804\n",
      "Epoch 38, mean squared loss: 182.5026228122612\n",
      "Epoch 39, mean squared loss: 182.02077429180824\n",
      "Epoch 40, mean squared loss: 183.11293211532558\n",
      "Epoch 41, mean squared loss: 184.06206820602492\n",
      "Epoch 42, mean squared loss: 182.16119237346192\n",
      "Epoch 43, mean squared loss: 184.05769512894562\n",
      "Epoch 44, mean squared loss: 186.43482543594035\n",
      "Epoch 45, mean squared loss: 182.45029930474408\n",
      "Epoch 46, mean squared loss: 182.91306274581663\n",
      "Epoch 47, mean squared loss: 182.57533570417502\n",
      "Epoch 48, mean squared loss: 182.3599778046265\n",
      "Epoch 49, mean squared loss: 181.93579023751744\n",
      "Epoch 50, mean squared loss: 182.48637578868565\n",
      "Epoch 51, mean squared loss: 182.61762960325174\n",
      "Epoch 52, mean squared loss: 182.03575732126393\n",
      "Epoch 53, mean squared loss: 183.82551904673363\n",
      "Epoch 54, mean squared loss: 183.447626104444\n",
      "Epoch 55, mean squared loss: 183.58632037802695\n",
      "Epoch 56, mean squared loss: 183.81928013405732\n",
      "Epoch 57, mean squared loss: 184.20967278277882\n",
      "Epoch 58, mean squared loss: 183.22816680916074\n",
      "Epoch 59, mean squared loss: 183.0665864839224\n",
      "Epoch 60, mean squared loss: 183.22556596668923\n",
      "Epoch 61, mean squared loss: 183.83830912086734\n",
      "Epoch 62, mean squared loss: 184.46508080909464\n",
      "Epoch 63, mean squared loss: 184.44834307909608\n",
      "Epoch 64, mean squared loss: 182.56319029912788\n",
      "Epoch 65, mean squared loss: 182.45307788791303\n",
      "Epoch 66, mean squared loss: 182.72110543407973\n",
      "Epoch 67, mean squared loss: 182.46769219545112\n",
      "Epoch 68, mean squared loss: 182.40266198252934\n",
      "Epoch 69, mean squared loss: 181.93721578022593\n",
      "Epoch 70, mean squared loss: 182.3728939244231\n",
      "Epoch 71, mean squared loss: 183.31026906134733\n",
      "Epoch 72, mean squared loss: 183.39862806467855\n",
      "Epoch 73, mean squared loss: 182.76077238058\n",
      "Epoch 74, mean squared loss: 182.80226679288748\n",
      "Epoch 75, mean squared loss: 182.93185760631076\n",
      "Epoch 76, mean squared loss: 182.51307916067123\n",
      "Epoch 77, mean squared loss: 183.03232781133372\n",
      "Epoch 78, mean squared loss: 183.5066758639138\n",
      "Epoch 79, mean squared loss: 183.59168952358715\n",
      "Epoch 80, mean squared loss: 183.55094445719394\n",
      "Epoch 81, mean squared loss: 182.61326627313062\n",
      "Epoch 82, mean squared loss: 182.58794154396054\n",
      "Epoch 83, mean squared loss: 182.78562004369266\n",
      "Epoch 84, mean squared loss: 183.6880220334893\n",
      "Epoch 85, mean squared loss: 182.17332085084578\n",
      "Epoch 86, mean squared loss: 182.3025229737457\n",
      "Epoch 87, mean squared loss: 182.92044482924592\n",
      "Epoch 88, mean squared loss: 183.04369254570793\n",
      "Epoch 89, mean squared loss: 182.19153670987959\n",
      "Epoch 90, mean squared loss: 182.3849377429289\n",
      "Epoch 91, mean squared loss: 181.65171455233903\n",
      "Epoch 92, mean squared loss: 184.00644850815405\n",
      "Epoch 93, mean squared loss: 183.3213530692309\n",
      "Epoch 94, mean squared loss: 183.1613114537715\n",
      "Epoch 95, mean squared loss: 183.4035274273232\n",
      "Epoch 96, mean squared loss: 184.89357318966972\n",
      "Epoch 97, mean squared loss: 183.0352057589788\n",
      "Epoch 98, mean squared loss: 184.00411863051247\n",
      "Epoch 99, mean squared loss: 183.6521259666334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn = Model([5, 5, 46], False)\n",
    "\n",
    "cnn.train(X_train, y_train, 100, 0.01, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  1000 countright:  18 . Accuracy:  1.7999999999999998 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn.validate(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1)\n",
      "(5, 1)\n",
      "(5, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Buffer has wrong number of dimensions (expected 1, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[874], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)))\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m index \u001b[38;5;241m=\u001b[39m cnn\u001b[38;5;241m.\u001b[39mpred(img) \u001b[38;5;66;03m#.index(max(cnn.pred(img)))\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m new_english_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdict.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     34\u001b[0m update_json_file(new_hindi_value, new_english_value)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Print the new values (for debug purposes)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# print(f'Updated JSON file with hindi: {new_hindi_value}, english: {new_english_value}')\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# read in and resize img\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# read in image for debugging\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexing.py:1692\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1690\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m-> 1692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexing.py:1089\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m section\n\u001b[1;32m   1088\u001b[0m         \u001b[38;5;66;03m# This is an elided recursive call to iloc/loc\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot applicable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexing.py:1743\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_list_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1747\u001b[0m     key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexing.py:1714\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;124;03mReturn Series values by list or array of integers.\u001b[39;00m\n\u001b[1;32m   1699\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;124;03m`axis` can only be zero.\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:4150\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4139\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   4140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   4141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   4143\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4148\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   4149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4150\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4151\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[1;32m   4152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:4130\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4125\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[1;32m   4126\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m   4127\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[1;32m   4128\u001b[0m     )\n\u001b[0;32m-> 4130\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4132\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4136\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4137\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[1;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m    689\u001b[0m             indexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    696\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/managers.py:773\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m             bp \u001b[38;5;241m=\u001b[39m BlockPlacement(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, sllen))\n\u001b[1;32m    772\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 773\u001b[0m                 \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mslobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m             ]\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sl_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    782\u001b[0m     blknos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblknos[slobj]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/array_algos/take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/array_algos/take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[1;32m    161\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[1;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32mpandas/_libs/algos_take_helper.pxi:2075\u001b[0m, in \u001b[0;36mpandas._libs.algos.take_1d_object_object\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Buffer has wrong number of dimensions (expected 1, got 2)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "cnn = Model([5, 5, 46], True)\n",
    "\"\"\"outs = cnn.pred(cnn.img_to_np(img_path))\n",
    "index = outs.index(max(outs))\n",
    "new_english_value = pd.read_csv(os.path.join(workspace, 'data', 'dict.csv'))[index][1]\"\"\"\n",
    "\n",
    "def update_json_file(hindi_value, english_value):\n",
    "    with open(out_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    data['hindi'] = hindi_value\n",
    "    data['english'] = english_value\n",
    "    with open(out_file, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# Define a list of possible values if applicable\n",
    "hindi_values = [\"\", \"\", \"\", \"\"]\n",
    "english_values = [\"Eng\", \"New\", \"Value\", \"Example\"]\n",
    "\n",
    "# Loop to update the JSON file every second with a random value\n",
    "try:\n",
    "    while True:\n",
    "        # Generate a random value from the lists\n",
    "        new_hindi_value = random.choice(hindi_values)\n",
    "        #new_english_value = random.choice(english_values)\n",
    "        img = np.array(Image.open(img_path).convert(\"L\").resize((32, 32))).flatten().reshape(1024, 1)\n",
    "        index = cnn.pred(img) #.index(max(cnn.pred(img)))\n",
    "        new_english_value = str(pd.read_csv(os.path.join(workspace, 'data', 'dict.csv')).iloc[index, 1])\n",
    "\n",
    "        update_json_file(new_hindi_value, new_english_value)\n",
    "        \n",
    "        # Print the new values (for debug purposes)\n",
    "        # print(f'Updated JSON file with hindi: {new_hindi_value}, english: {new_english_value}')\n",
    "\n",
    "        # read in and resize img\n",
    " \n",
    "        # read in image for debugging\n",
    "        \"\"\"img.show(\"img to classify\")\n",
    "        print(img.format)\n",
    "        print(img.mode)\n",
    "        print(img.size)\"\"\"\n",
    "\n",
    "        # create np array and resize\n",
    "        \"\"\"img_data = np.array(img).flatten()\n",
    "        print(img_data.shape)\n",
    "        print(img_data)\"\"\"\n",
    "\n",
    "        \n",
    "        time.sleep(3)  # Wait for 1 second\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Update stopped by user.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
