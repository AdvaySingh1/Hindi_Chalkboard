{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "# import for time, and file change\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# frequent updating\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filepaths\n",
    "workspace = \"/Users/advaysingh/Documents/projects/hindi_classification/\" \n",
    "#print(\"Current workspace:\", workspace) /Users/advaysingh/Documents/projects/hindi_classification/server/snapshot.png\n",
    "\n",
    "data = os.path.join(workspace, 'data/Hindi/')\n",
    "dict_lib = os.path.join(workspace, 'data/dict.csv')\n",
    "img_path = os.path.join(workspace, 'server/snapshot.png')\n",
    "out_file = os.path.join(workspace, 'server/outputs.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/test dicts with files and labels\n",
    "\n",
    "def create_dict(x: str) -> dict:\n",
    "    x_dict = {}\n",
    "    i = 0\n",
    "    for dir in os.listdir(os.path.join(data, x)):\n",
    "        for file in os.listdir(os.path.join(data, x, dir)):\n",
    "            x_dict[os.path.join(data, x, dir, file)] = i\n",
    "        i += 1\n",
    "    return x_dict\n",
    "\n",
    "# make pandas df\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(create_dict('Train'), orient='index')\n",
    "test_df = pd.DataFrame.from_dict(create_dict('Test'), orient='index')\n",
    "\n",
    "#print(train_df)\n",
    "#df_temp = train_df[0].drop_duplicates()\n",
    "# print(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057317038046, 0.24472847105479767, 0.6652409557748219]\n"
     ]
    }
   ],
   "source": [
    "# Activations class\n",
    "class Activation:\n",
    "    # can add more types\n",
    "    def __init__(self, act_type: str) -> None:\n",
    "        self.act = act_type\n",
    "\n",
    "    def print_act(self) -> None:\n",
    "        return self.act\n",
    "    \n",
    "    def compute(self, z) -> list:\n",
    "\n",
    "        # Sigmoid function\n",
    "        if (self.act == 'sigmoid'):\n",
    "            n = []\n",
    "            for val in (z):\n",
    "                n.append(1.0 / (1.0 + np.exp(val)))\n",
    "            return n\n",
    "        \n",
    "        # ReLU function\n",
    "        elif (self.act == 'relu'):\n",
    "            vals = []\n",
    "            for val in z:\n",
    "                if val < 0.0:\n",
    "                    vals.append(0.0)\n",
    "                else:\n",
    "                    vals.append(1.0)\n",
    "            return vals\n",
    "\n",
    "    def prime(self, z: list):\n",
    "\n",
    "        # Sigmoid prime\n",
    "        if (self.act == 'sigmoid'):\n",
    "            sigs = self.compute(z)\n",
    "            vals = []\n",
    "            for val in sigs:\n",
    "                vals.append(val * (1 - val))\n",
    "            return vals\n",
    "        \n",
    "        # ReLU prime\n",
    "        if (self.act == 'relu'):\n",
    "            vals = []\n",
    "            for val in z:\n",
    "                if val == 0:\n",
    "                    vals.append(0.0)\n",
    "                else:\n",
    "                    vals.append(1.0)\n",
    "            return vals\n",
    "\n",
    "\n",
    "def softmax(costs: list) -> list:\n",
    "    exp_vals = []\n",
    "    for cost in np.array(costs):\n",
    "        exp_vals.append(np.exp(cost))\n",
    "    return_vals = []\n",
    "    for i in range(len(exp_vals)):\n",
    "        return_vals.append(exp_vals[i] / sum(exp_vals))\n",
    "    return return_vals\n",
    "\n",
    "def softmax_prime(costs: list) -> list:\n",
    "    softs = softmax(costs)\n",
    "    vals = []\n",
    "    for val in softs:\n",
    "        vals.append(val * (1 - val))\n",
    "    return vals\n",
    "\n",
    "n = softmax([2, 3, 4])\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(z: np.array) -> np.array:\n",
    "    \"\"\"Compute the sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def act_prime(z: np.array) -> np.array:\n",
    "    \"\"\"Compute the derivative of the sigmoid activation function.\"\"\"\n",
    "    ap = activate(z)\n",
    "    return ap * (1 - ap)\n",
    "\n",
    "def softmax(z: np.array) -> np.array:\n",
    "    e_z = np.exp(z - np.max(z)) # Improved stability\n",
    "    return e_z / e_z.sum(axis=0)\n",
    "\n",
    "def softmax_prime(z: np.array) -> np.array:\n",
    "    softmax_vals = softmax(z)\n",
    "    # For each i, j in softmax output vector size: [si * (1 - si)] if i == j, otherwise: -si * sj\n",
    "    return softmax_vals * (1 - softmax_vals)\n",
    "    #return np.diag(softmax_vals) - np.outer(softmax_vals, softmax_vals)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, layers: list, load_perams: bool) -> None:\n",
    "        self.train_df = train_df\n",
    "        self.layers = layers\n",
    "\n",
    "        self.all_weights = [None] * len(self.layers)\n",
    "        self.all_bias = [None] * len(self.layers)\n",
    "\n",
    "        # read in weights if load perams\n",
    "        if (load_perams):\n",
    "            layer = 0\n",
    "            weightlist = os.listdir(os.path.join(workspace, 'data', 'Hyper_p/Weights'))\n",
    "            for file in sorted(weightlist, key=lambda s: s.lower()):\n",
    "                self.all_weights[layer] = (pd.read_csv(os.path.join(workspace, 'data', 'Hyper_p/Weights', file)).to_numpy())\n",
    "                layer += 1\n",
    "            layer = 0\n",
    "            biaslist = os.listdir(os.path.join(workspace, 'data', 'Hyper_p/Biases'))\n",
    "            for file in sorted(biaslist, key=lambda s: s.lower()):\n",
    "                self.all_bias[layer] = (pd.read_csv(os.path.join(workspace, 'data', 'Hyper_p/Biases', file)).to_numpy())\n",
    "                layer += 1\n",
    "\n",
    "        else:\n",
    "            for i in range(len(layers)):\n",
    "                self.all_weights[i] = self.random_arrs(i)\n",
    "                if i == 0:\n",
    "                    self.all_bias[i] = np.random.uniform(low=-0.03125, high=0.03125, size=(self.layers[i],1))\n",
    "                else:\n",
    "                    bias_val = 0.5 # TODO reconsider\n",
    "                    self.all_bias[i] = np.random.uniform(low=-bias_val, high=bias_val, size=(self.layers[i],1))\n",
    "    \n",
    "\n",
    "    def random_arrs(self, layer: int):\n",
    "        if (layer == 0):\n",
    "            return(np.random.uniform(low=-0.03125, high=0.03125, size=(self.layers[layer],1024)))\n",
    "        else:\n",
    "            weight_val = 1 / math.sqrt(self.layers[layer - 1])\n",
    "            return(np.random.uniform(low=-weight_val, high=weight_val, size=(self.layers[layer], self.layers[layer-1])))\n",
    "\n",
    "    def img_to_np(self, dir) -> np.array:\n",
    "        return np.array(Image.open(dir)).flatten()\n",
    "    \n",
    "    def prop_forward(self, x: np.ndarray):\n",
    "        activations = [x] #activations.size > zs\n",
    "        zs = []\n",
    "        for i in range(len(self.layers)):\n",
    "            x = np.matmul(self.all_weights[i], x) + self.all_bias[i]\n",
    "            zs.append(x)\n",
    "            if (i == len(self.layers) - 1):\n",
    "                x = softmax(x)\n",
    "            else:\n",
    "                x = activate(x)\n",
    "            activations.append(x)\n",
    "        return activations, zs\n",
    "\n",
    "\n",
    "    \n",
    "    def prop_backward(self, x_batch: np.ndarray, y_batch: np.ndarray):\n",
    "        # Initialize gradients as zero\n",
    "        delta_b_sum = [np.zeros(b.shape) for b in self.all_bias]\n",
    "        delta_w_sum = [np.zeros(w.shape) for w in self.all_weights]\n",
    "\n",
    "        # Loop over each example in the batch\n",
    "        for x, y in zip(x_batch, y_batch):\n",
    "            activations, zs = self.prop_forward(x)\n",
    "            delta = (activations[-1] - y.reshape(46,1)) #* softmax_prime(zs[-1])\n",
    "\n",
    "            # Gradients for output layer\n",
    "            delta_b_sum[-1] += delta\n",
    "            delta_w_sum[-1] += np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "            # Gradients for hidden layers\n",
    "            for i in range(2, len(self.layers)):\n",
    "                z = zs[-i]\n",
    "                sp = act_prime(z)\n",
    "                delta = np.dot(self.all_weights[-i + 1].transpose(), delta) * sp\n",
    "                delta_b_sum[-i] += delta\n",
    "                delta_w_sum[-i] += np.dot(delta, activations[-i - 1].T)\n",
    "\n",
    "        # Average the gradients over the mini-batch\n",
    "        num_examples = len(x_batch)\n",
    "        delta_b_avg = [db_sum / num_examples for db_sum in delta_b_sum]\n",
    "        delta_w_avg = [dw_sum / num_examples for dw_sum in delta_w_sum]\n",
    "\n",
    "        # Return the average gradients\n",
    "        return delta_w_avg, delta_b_avg\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, learning_rate):\n",
    "        # aggregate gradients from the mini_batch\n",
    "        delta_w, delta_b = self.prop_backward(mini_batch[0], mini_batch[1])\n",
    "    \n",
    "        # update weights and biases\n",
    "        self.all_weights = [w - (learning_rate * dw) for w, dw in zip(self.all_weights, delta_w)]\n",
    "        self.all_bias = [b - (learning_rate * db) for b, db in zip(self.all_bias, delta_b)]\n",
    "    \n",
    "\n",
    "    def train(self, X_train, y_train, epochs, learning_rate, mini_batch_size, test_df):\n",
    "        n = len(X_train)\n",
    "\n",
    "        # Convert to NumPy arrays if necessary\n",
    "        X_train_np = np.array(X_train) if not isinstance(X_train, np.ndarray) else X_train\n",
    "        y_train_np = np.array(y_train) if not isinstance(y_train, np.ndarray) else y_train\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data for each epoch\n",
    "            permutation = np.random.permutation(n)\n",
    "            X_train_shuffled = X_train_np[permutation]\n",
    "            y_train_shuffled = y_train_np[permutation]\n",
    "        \n",
    "            # Partition training data into mini-batches\n",
    "            mini_batches = [\n",
    "                (X_train_shuffled[k:k+mini_batch_size], y_train_shuffled[k:k+mini_batch_size])\n",
    "                for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "        \n",
    "            # Update the model's weights with each mini-batch\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, learning_rate)\n",
    "            \n",
    "            if(epoch % 20 == 0):\n",
    "                self.validate(test_df)\n",
    "\n",
    "            new_loss = self.calc_squared_loss(X_train, y_train)\n",
    "            print(f\"Epoch {epoch}, mean squared loss: {new_loss}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    def calc_squared_loss(self, X, y):\n",
    "        squared_errors = []\n",
    "        for i in range(len(X)):\n",
    "            y_pred, _ = self.prop_forward(X[i].reshape(1024, 1))  # prop_forward should give the prediction\n",
    "            squared_error = np.sum((y[i].reshape(46, 1) - y_pred[-1]) ** 2) / 2 \n",
    "            squared_errors.append(squared_error)\n",
    "        \n",
    "        # Calculate mean squared error over all examples.\n",
    "        mse = np.mean(squared_errors)\n",
    "        return mse\n",
    "\n",
    "    def pred(self, img: np.array):\n",
    "        img, _ = self.prop_forward(img)\n",
    "        #img, _ = self.prop_forward(np.array(img).reshape(len(img), 1))\n",
    "        return (img[-1])\n",
    "    \n",
    "    def vals_for_softmax(self, index: int) -> np.array:\n",
    "        \"\"\" Helper function to create softmax values\"\"\"\n",
    "        vals = np.zeros(46)\n",
    "        vals[index] = 1\n",
    "        vals.reshape(46, 1)\n",
    "        return vals\n",
    "    \n",
    "    def validate(self, test_df: pd.DataFrame) -> None:\n",
    "        count, countright = 0, 0\n",
    "        for img_dir, row in test_df.iterrows():\n",
    "            img = np.array(Image.open(img_dir).convert(\"L\").resize((32, 32))).flatten().reshape(1024, 1)\n",
    "            outputs = self.pred(img)\n",
    "            actual = self.vals_for_softmax(row[0])\n",
    "            if (outputs.argmax() - actual.argmax() == 0):\n",
    "                countright += 1\n",
    "            count += 1\n",
    "            accuracy = (float(countright) / float(count)) * 100\n",
    "        print(\"count: \", count, \". Countright: \", countright, \" Accuracy: \", accuracy, \"%\")\n",
    "\n",
    "    def save(self) -> None:\n",
    "        hp_data = '/Users/advaysingh/Documents/projects/hindi_classification/data/hyper_p'\n",
    "        hp_data_biases, hp_data_weights = hp_data + '/Biases', hp_data + '/Weights'\n",
    "        os.makedirs(hp_data)\n",
    "        os.makedirs(hp_data_biases)\n",
    "        os.makedirs(hp_data_weights)\n",
    "        for layer in range(len(self.layers)):\n",
    "            df_weights = pd.DataFrame(self.all_weights[layer])\n",
    "            df_bias = pd.DataFrame(self.all_bias[layer])\n",
    "            weights_file_name = \"layer_\" + str(layer) + \"weights.csv\"\n",
    "            bias_file_name = \"layer_\" + str(layer) + \"bias.csv\"\n",
    "            weights_file = open(os.path.join(hp_data_weights, weights_file_name), \"x\")\n",
    "            bias_file = open(os.path.join(hp_data_biases, bias_file_name), \"x\")\n",
    "            df_weights.to_csv(weights_file, index=False)\n",
    "            df_bias.to_csv(bias_file, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vals_for_softmax(index: int) -> np.array:\n",
    "        \"\"\" Helper function to create softmax values\"\"\"\n",
    "        vals = np.zeros(46)\n",
    "        vals[index] = 1\n",
    "        vals.reshape(46, 1)\n",
    "        return vals\n",
    "test_df_sample = test_df.sample(500)\n",
    "\n",
    "\n",
    "X_train, y_train = [], []\n",
    "for img_dir, row in train_df.iterrows():\n",
    "            X_train.append(np.array(Image.open(img_dir)).flatten().reshape(1024, 1))\n",
    "            y_train.append(vals_for_softmax(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  13800 . Countright:  10761  Accuracy:  77.97826086956522 %\n",
      "Epoch 0, mean squared loss: 0.10321868690908437\n",
      "Epoch 1, mean squared loss: 0.1030527858201615\n",
      "Epoch 2, mean squared loss: 0.1030437913466607\n",
      "Epoch 3, mean squared loss: 0.10296387029703505\n",
      "Epoch 4, mean squared loss: 0.10298123533499003\n",
      "Epoch 5, mean squared loss: 0.10278212948414452\n",
      "Epoch 6, mean squared loss: 0.10266401845953697\n",
      "Epoch 7, mean squared loss: 0.10257130002332329\n",
      "Epoch 8, mean squared loss: 0.1024809638247909\n",
      "Epoch 9, mean squared loss: 0.10250868258677273\n",
      "Epoch 10, mean squared loss: 0.10244536652037599\n",
      "Epoch 11, mean squared loss: 0.10226417207285078\n",
      "Epoch 12, mean squared loss: 0.1022764648041767\n",
      "Epoch 13, mean squared loss: 0.10209165745843679\n",
      "Epoch 14, mean squared loss: 0.10209142133349025\n",
      "Epoch 15, mean squared loss: 0.10203671531038519\n",
      "Epoch 16, mean squared loss: 0.1019126948242253\n",
      "Epoch 17, mean squared loss: 0.10205173214467311\n",
      "Epoch 18, mean squared loss: 0.10180988176797792\n",
      "Epoch 19, mean squared loss: 0.10179410757355904\n",
      "count:  13800 . Countright:  10755  Accuracy:  77.93478260869566 %\n",
      "Epoch 20, mean squared loss: 0.10183497163204475\n",
      "Epoch 21, mean squared loss: 0.10156247112742318\n",
      "Epoch 22, mean squared loss: 0.10159147467301377\n",
      "Epoch 23, mean squared loss: 0.101508987005211\n",
      "Epoch 24, mean squared loss: 0.10149528706144842\n",
      "Epoch 25, mean squared loss: 0.10134638674914706\n",
      "Epoch 26, mean squared loss: 0.10141123094626683\n",
      "Epoch 27, mean squared loss: 0.10120497141207839\n",
      "Epoch 28, mean squared loss: 0.1011329700422132\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1026], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cnn \u001b[38;5;241m=\u001b[39m Model([\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m46\u001b[39m], \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1025], line 136\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, X_train, y_train, epochs, learning_rate, mini_batch_size, test_df)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Update the model's weights with each mini-batch\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mini_batch \u001b[38;5;129;01min\u001b[39;00m mini_batches:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(test_df)\n",
      "Cell \u001b[0;32mIn[1025], line 108\u001b[0m, in \u001b[0;36mModel.update_mini_batch\u001b[0;34m(self, mini_batch, learning_rate)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_mini_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, mini_batch, learning_rate):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# aggregate gradients from the mini_batch\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     delta_w, delta_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprop_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# update weights and biases\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_weights \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;241m-\u001b[39m (learning_rate \u001b[38;5;241m*\u001b[39m dw) \u001b[38;5;28;01mfor\u001b[39;00m w, dw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_weights, delta_w)]\n",
      "Cell \u001b[0;32mIn[1025], line 83\u001b[0m, in \u001b[0;36mModel.prop_backward\u001b[0;34m(self, x_batch, y_batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Loop over each example in the batch\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x_batch, y_batch):\n\u001b[0;32m---> 83\u001b[0m     activations, zs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprop_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     delta \u001b[38;5;241m=\u001b[39m (activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m46\u001b[39m,\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m#* softmax_prime(zs[-1])\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Gradients for output layer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1025], line 65\u001b[0m, in \u001b[0;36mModel.prop_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m zs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[0;32m---> 65\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_bias[i]\n\u001b[1;32m     66\u001b[0m     zs\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "cnn = Model([300, 100, 46], True)\n",
    "cnn.train(X_train, y_train, 5000, 0.05, (300), test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  500 . Countright:  433  Accuracy:  86.6 %\n"
     ]
    }
   ],
   "source": [
    "df_sample = train_df.sample(500)\n",
    "cnn = Model([300, 100, 46], True)\n",
    "cnn.validate(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "An unexpected error occurred: 'numpy.ndarray' object has no attribute 'convert'\n",
      "Update stopped by user.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "cnn = Model([300, 100, 46], True)\n",
    "\"\"\"outs = cnn.pred(cnn.img_to_np(img_path))\n",
    "index = outs.index(max(outs))\n",
    "new_english_value = pd.read_csv(os.path.join(workspace, 'data', 'dict.csv'))[index][1]\"\"\"\n",
    "\n",
    "def update_json_file(hindi_value, english_value):\n",
    "    with open(out_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    data['hindi'] = hindi_value\n",
    "    data['english'] = english_value\n",
    "    with open(out_file, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# Define a list of possible values if applicable\n",
    "hindi_values = [\"हिं\", \"नया\", \"मूल्य\", \"उदाहरण\"]\n",
    "english_values = [\"Eng\", \"New\", \"Value\", \"Example\"]\n",
    "\n",
    "# Loop to update the JSON file every second with a random value\n",
    "try:\n",
    "    while True:\n",
    "        # Generate a random value from the lists\n",
    "        #new_hindi_value = random.choice(hindi_values)\n",
    "        #new_english_value = random.choice(english_values)\n",
    "        try:\n",
    "            img = np.array(Image.open(img_path).resize((32, 32)).convert('L')).flatten().reshape(1024, 1)\n",
    "            index = list(cnn.pred(img)).index(list(max(cnn.pred(img)))) #.index(max(cnn.pred(img)))\n",
    "            new_english_value = str(pd.read_csv(os.path.join(workspace, 'data', 'dict.csv')).iloc[index -1, 1])\n",
    "            new_hindi_value = str(pd.read_csv(os.path.join(workspace, 'data', 'dict.csv')).iloc[index -1, 2])\n",
    "\n",
    "            update_json_file(new_hindi_value, new_english_value)\n",
    "\n",
    "            \n",
    "        except SyntaxError as e:\n",
    "            print(f\"An error occurred while processing the image: {e}\")\n",
    "            new_english_value = None  # Set a default value or handle differently\n",
    "            new_hindi_value = None\n",
    "        except Exception as e:\n",
    "            # Optionally, catch and handle other non-SyntaxErrors if necessary\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            new_english_value = None  # Set a default value or handle differently\n",
    "            new_hindi_value = None\n",
    "        \n",
    "        # Print the new values (for debug purposes)\n",
    "        # print(f'Updated JSON file with hindi: {new_hindi_value}, english: {new_english_value}')\n",
    "\n",
    "        # read in and resize img\n",
    " \n",
    "        # read in image for debugging\n",
    "        \"\"\"img.show(\"img to classify\")\n",
    "        print(img.format)\n",
    "        print(img.mode)\n",
    "        print(img.size)\"\"\"\n",
    "\n",
    "        # create np array and resize\n",
    "        \"\"\"img_data = np.array(img).flatten()\n",
    "        print(img_data.shape)\n",
    "        print(img_data)\"\"\"\n",
    "\n",
    "        \n",
    "        time.sleep(0.1)  # Wait for 0.5 seconds\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Update stopped by user.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
