{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "# import for time, and file change\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# frequent updating\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filepaths\n",
    "workspace = \"/Users/advaysingh/Documents/projects/hindi_classification/\" \n",
    "#print(\"Current workspace:\", workspace)\n",
    "\n",
    "data = os.path.join(workspace, 'data/Hindi/')\n",
    "dict_lib = os.path.join(workspace, 'data/dict.csv')\n",
    "img_path = os.path.join(workspace, 'server/snapshot.png')\n",
    "out_file = os.path.join(workspace, 'server/outputs.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/test dicts with files and labels\n",
    "\n",
    "def create_dict(x: str) -> dict:\n",
    "    x_dict = {}\n",
    "    i = 0\n",
    "    for dir in os.listdir(os.path.join(data, x)):\n",
    "        for file in os.listdir(os.path.join(data, x, dir)):\n",
    "            x_dict[os.path.join(data, x, dir, file)] = i\n",
    "        i += 1\n",
    "    return x_dict\n",
    "\n",
    "# make pandas df\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(create_dict('Train'), orient='index')\n",
    "test_df = pd.DataFrame.from_dict(create_dict('Test'), orient='index')\n",
    "\n",
    "#print(train_df)\n",
    "#df_temp = train_df[0].drop_duplicates()\n",
    "# print(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057317038046, 0.24472847105479767, 0.6652409557748219]\n"
     ]
    }
   ],
   "source": [
    "# Activations class\n",
    "class Activation:\n",
    "    # can add more types\n",
    "    def __init__(self, act_type: str) -> None:\n",
    "        self.act = act_type\n",
    "\n",
    "    def print_act(self) -> None:\n",
    "        return self.act\n",
    "    \n",
    "    def compute(self, z) -> list:\n",
    "\n",
    "        # Sigmoid function\n",
    "        if (self.act == 'sigmoid'):\n",
    "            n = []\n",
    "            for val in (z):\n",
    "                n.append(1.0 / (1.0 + np.exp(val)))\n",
    "            return n\n",
    "        \n",
    "        # ReLU function\n",
    "        elif (self.act == 'relu'):\n",
    "            vals = []\n",
    "            for val in z:\n",
    "                if val < 0.0:\n",
    "                    vals.append(0.0)\n",
    "                else:\n",
    "                    vals.append(1.0)\n",
    "            return vals\n",
    "\n",
    "    def prime(self, z: list):\n",
    "\n",
    "        # Sigmoid prime\n",
    "        if (self.act == 'sigmoid'):\n",
    "            sigs = self.compute(z)\n",
    "            vals = []\n",
    "            for val in sigs:\n",
    "                vals.append(val * (1 - val))\n",
    "            return vals\n",
    "        \n",
    "        # ReLU prime\n",
    "        if (self.act == 'relu'):\n",
    "            vals = []\n",
    "            for val in z:\n",
    "                if val == 0:\n",
    "                    vals.append(0.0)\n",
    "                else:\n",
    "                    vals.append(1.0)\n",
    "            return vals\n",
    "\n",
    "\n",
    "def softmax(costs: list) -> list:\n",
    "    exp_vals = []\n",
    "    for cost in np.array(costs):\n",
    "        exp_vals.append(np.exp(cost))\n",
    "    return_vals = []\n",
    "    for i in range(len(exp_vals)):\n",
    "        return_vals.append(exp_vals[i] / sum(exp_vals))\n",
    "    return return_vals\n",
    "\n",
    "def softmax_prime(costs: list) -> list:\n",
    "    softs = softmax(costs)\n",
    "    vals = []\n",
    "    for val in softs:\n",
    "        vals.append(val * (1 - val))\n",
    "    return vals\n",
    "\n",
    "n = softmax([2, 3, 4])\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"class Model:\n",
    "    def __init__(self, train_df: pd.DataFrame,\n",
    "                  act: Activation, layers: list, step: float, stochastic: bool, load_perams: bool) -> None:\n",
    "        self.train_df = train_df\n",
    "\n",
    "        # create activation function\n",
    "        self.act = act\n",
    "        self.layers = layers\n",
    "\n",
    "        # init weight and bias with # layers np arrays\n",
    "        # Using the Xavier weight init method https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "        self.all_weights = [None] * (int(len(layers)))\n",
    "        self.all_bias = [None] * (int(len(layers)))\n",
    "        if (load_perams):\n",
    "            layer = 0\n",
    "            weightlist = os.listdir(os.path.join(workspace, 'data', 'Hyper_p/Weights'))\n",
    "            for file in sorted(weightlist, key=lambda s: s.lower()):\n",
    "                self.all_weights[layer] = pd.read_csv(os.path.join(workspace, 'data', 'Hyper_p/Weights', file)).to_numpy()\n",
    "                #self.all_weights[layer] = np.delete(self.all_weights, [0], axis=1)\n",
    "                print(self.all_weights[layer].shape)\n",
    "                layer += 1\n",
    "            layer = 0\n",
    "            biaslist = os.listdir(os.path.join(workspace, 'data', 'Hyper_p/Biases'))\n",
    "            for file in sorted(biaslist, key=lambda s: s.lower()):\n",
    "                self.all_bias[layer] = pd.read_csv(os.path.join(workspace, 'data', 'Hyper_p/Biases', file)).to_numpy()\n",
    "                layer += 1\n",
    "            print(self.all_weights[0])\n",
    "        else:\n",
    "            for i in range(len(layers)):\n",
    "                self.all_weights[i] = self.random_arrs(i)\n",
    "                if i == 0:\n",
    "                    self.all_bias[i] = np.random.uniform(low=-0.03125, high=0.03125, size=(self.layers[i],1))\n",
    "                else:\n",
    "                    bias_val = 1 / math.sqrt(self.layers[i - 1])\n",
    "                    self.all_bias[i] = np.random.uniform(low=-bias_val, high=bias_val, size=(self.layers[i],1))\n",
    "            #for val in self.all_weights:\n",
    "               # print(val.shape)\n",
    "\n",
    "        # init learn speed\n",
    "        self.speed = step\n",
    "\n",
    "        # determine gradient decent type\n",
    "        self.stochastic = stochastic\n",
    "\n",
    "    def random_arrs(self, layer: int):\n",
    "        if (layer == 0):\n",
    "            return(np.random.uniform(low=-0.03125, high=0.03125, size=(self.layers[layer],1024)))\n",
    "        else:\n",
    "            weight_val = 1 / math.sqrt(self.layers[layer - 1])\n",
    "            return(np.random.uniform(low=-weight_val, high=weight_val, size=(self.layers[layer], self.layers[layer-1])))\n",
    "\n",
    "    def img_to_np(self, dir) -> np.array:\n",
    "        return np.array(Image.open(dir)).flatten()\n",
    "\n",
    "\n",
    "    def print_weights(self) -> None:\n",
    "        for arr in self.all_weights:\n",
    "            print(arr.shape)\n",
    "\n",
    "    def prop_forward(self, inputs: list, layer: int) -> list:\n",
    "        a_vals = [0.0] * int(self.layers[layer])\n",
    "        a_vals = np.dot(self.all_weights[layer], pd.DataFrame(inputs)) + self.all_bias[layer]\n",
    "        if (layer == len(self.layers) - 1):\n",
    "            return (softmax(a_vals))\n",
    "        return(self.act.compute(a_vals))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def prop_backword(self, x: list, label: list):\n",
    "        delta_w = [None] * len(self.layers)\n",
    "        delta_b = [None] * len(self.layers)\n",
    "        inputs = []\n",
    "        for i in range(len(self.layers)):\n",
    "            inputs.append(x)\n",
    "            x = self.prop_forward(x, i)\n",
    "        error = [a - b for a, b in zip(x, label)]\n",
    "        vals = np.array([a * b for a, b in zip(error, softmax_prime(x))])\n",
    "        for i in range(len(self.layers), 0, -1):\n",
    "            delta_w[i - 1] = np.dot(pd.DataFrame(vals), pd.DataFrame(inputs[i - 1]).transpose())\n",
    "            delta_b[i - 1] = vals\n",
    "            vals = np.dot(pd.DataFrame(self.all_weights[i - 1]).transpose(), pd.DataFrame(vals))\n",
    "            vals = [a * b for a, b in zip(vals, self.act.prime(inputs[i - 1]))]\n",
    "            #print(np.dot(pd.DataFrame(vals), pd.DataFrame(inputs[0]).transpose()))\n",
    "        #print(delta_w[0])\n",
    "        return delta_w, delta_b\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    # potentially reconsider for time complexity\n",
    "    def vals_for_softmax(self, index: int) -> list:\n",
    "        vals = self.train_df[0].unique()\n",
    "        for i in range(len(vals)):\n",
    "            if vals[i] != index:\n",
    "                vals[i] = 0\n",
    "            else:\n",
    "                vals[i] = 1\n",
    "        return vals\n",
    "\n",
    "\n",
    "    def train(self, epochs: int) -> None:\n",
    "        length_set = len(self.train_df)\n",
    "        for epoch in  range(epochs):\n",
    "            delta_w = [np.zeros_like(array) for array in self.all_weights]\n",
    "            delta_b = [np.zeros_like(array) for array in self.all_bias]\n",
    "            count = 0\n",
    "            for img_dir, row in self.train_df.iterrows():\n",
    "                print(\"img\", count)\n",
    "                count += 1\n",
    "                temp_w, temp_b = self.prop_backword(self.img_to_np(img_dir).tolist(), self.vals_for_softmax(row[0]))\n",
    "                for i in range(len(self.layers)):\n",
    "                    delta_b[i] += temp_b[i]\n",
    "                    delta_w[i] += temp_w[i]\n",
    "            delta_w = [array / length_set for array in delta_w]\n",
    "            delta_b = [np.array(array) / length_set for array in delta_b]\n",
    "            for i in range(len(self.layers)):\n",
    "                self.all_weights[i] += delta_w[i]\n",
    "                self.all_bias[i] += delta_b[i]\n",
    "            print(\"validating\")\n",
    "            print(\"epoch: \", epoch)\n",
    "            self.validate(self.train_df)\n",
    "\n",
    "    def pred(self, img: np.array) -> int:\n",
    "        for i in range(len(self.layers)):\n",
    "            img = self.prop_forward(img, i)\n",
    "        return img\n",
    "\n",
    "    def validate(self, test_df: pd.DataFrame) -> None:\n",
    "        count, countright = 0, 0\n",
    "        for img_dir, row in test_df.iterrows():\n",
    "            outputs = self.pred(self.img_to_np(img_dir))\n",
    "            actual = list(self.vals_for_softmax(row[0]))\n",
    "            if (outputs.index(max(outputs))) == (actual.index(max(actual))):\n",
    "                countright += 1\n",
    "            count += 1\n",
    "            accuracy = (float(countright) / float(count)) * 100\n",
    "        print(\"count: \", count, \"countright: \", countright, \". Accuracy: \", accuracy, \"%\")\n",
    "\n",
    "    def save(self) -> None:\n",
    "        hp_data = '/Users/advaysingh/Documents/projects/hindi_classification/data/hyper_p'\n",
    "        hp_data_biases, hp_data_weights = hp_data + '/Biases', hp_data + '/Weights'\n",
    "        os.makedirs(hp_data)\n",
    "        os.makedirs(hp_data_biases)\n",
    "        os.makedirs(hp_data_weights)\n",
    "        for layer in range(len(self.layers)):\n",
    "            df_weights = pd.DataFrame(self.all_weights[layer])\n",
    "            df_bias = pd.DataFrame(self.all_bias[layer])\n",
    "            weights_file_name = \"layer_\" + str(layer) + \"weights.csv\"\n",
    "            bias_file_name = \"layer_\" + str(layer) + \"bias.csv\"\n",
    "            weights_file = open(os.path.join(hp_data_weights, weights_file_name), \"x\")\n",
    "            bias_file = open(os.path.join(hp_data_biases, bias_file_name), \"x\")\n",
    "            df_weights.to_csv(weights_file, index=False)\n",
    "            df_bias.to_csv(bias_file, index=False)\n",
    "\n",
    "\n",
    "    def print_weights(self, layer: int) -> None:\n",
    "        for row in self.all_weights[layer]:\n",
    "            for weight in row:\n",
    "                print(weight)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(z: np.array) -> np.array:\n",
    "    \"\"\"Compute the sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def act_prime(z: np.array) -> np.array:\n",
    "    \"\"\"Compute the derivative of the sigmoid activation function.\"\"\"\n",
    "    ap = activate(z)\n",
    "    return ap * (1 - ap)\n",
    "\n",
    "def softmax(z: np.array) -> np.array:\n",
    "    e_z = np.exp(z - np.max(z)) # Improved stability\n",
    "    return e_z / e_z.sum(axis=0)\n",
    "\n",
    "def softmax_prime(z: np.array) -> np.array:\n",
    "    softmax_vals = softmax(z)\n",
    "    # For each i, j in softmax output vector size: [si * (1 - si)] if i == j, otherwise: -si * sj\n",
    "    return np.diag(softmax_vals) - np.outer(softmax_vals, softmax_vals)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, layers: list, load_perams: bool) -> None:\n",
    "        self.train_df = train_df\n",
    "        self.layers = layers\n",
    "\n",
    "        self.all_weights = [None] * len(self.layers)\n",
    "        self.all_bias = [None] * len(self.layers)\n",
    "\n",
    "        # read in weights if load perams\n",
    "        if (load_perams):\n",
    "            layer = 0\n",
    "            weightlist = os.listdir(os.path.join(workspace, 'data', 'Hyper_p/Weights'))\n",
    "            for file in sorted(weightlist, key=lambda s: s.lower()):\n",
    "                self.all_weights[layer] = (pd.read_csv(os.path.join(workspace, 'data', 'Hyper_p/Weights', file)).to_numpy())\n",
    "                layer += 1\n",
    "            layer = 0\n",
    "            biaslist = os.listdir(os.path.join(workspace, 'data', 'Hyper_p/Biases'))\n",
    "            for file in sorted(biaslist, key=lambda s: s.lower()):\n",
    "                self.all_bias[layer] = (pd.read_csv(os.path.join(workspace, 'data', 'Hyper_p/Biases', file)).to_numpy())\n",
    "                layer += 1\n",
    "\n",
    "        else:\n",
    "            for i in range(len(layers)):\n",
    "                self.all_weights[i] = self.random_arrs(i)\n",
    "                if i == 0:\n",
    "                    self.all_bias[i] = np.random.uniform(low=-0.03125, high=0.03125, size=(self.layers[i],1))\n",
    "                else:\n",
    "                    bias_val = 0.5 # TODO reconsider\n",
    "                    self.all_bias[i] = np.random.uniform(low=-bias_val, high=bias_val, size=(self.layers[i],1))\n",
    "    \n",
    "\n",
    "    def random_arrs(self, layer: int):\n",
    "        if (layer == 0):\n",
    "            return(np.random.uniform(low=-0.03125, high=0.03125, size=(self.layers[layer],1024)))\n",
    "        else:\n",
    "            weight_val = 1 / math.sqrt(self.layers[layer - 1])\n",
    "            return(np.random.uniform(low=-weight_val, high=weight_val, size=(self.layers[layer], self.layers[layer-1])))\n",
    "\n",
    "    def img_to_np(self, dir) -> np.array:\n",
    "        return np.array(Image.open(dir)).flatten()\n",
    "    \n",
    "    def prop_forward(self, x: np.ndarray):\n",
    "        activations = [x] #activations.size > zs\n",
    "        zs = []\n",
    "        for i in range(len(self.layers)):\n",
    "            x = np.matmul(self.all_weights[i], x) + self.all_bias[i]\n",
    "            zs.append(x)\n",
    "            if (i == len(self.layers) - 1):\n",
    "                x = softmax(x)\n",
    "            else:\n",
    "                x = activate(x)\n",
    "            activations.append(x)\n",
    "        return activations, zs\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"def prop_backward(self, x: np.ndarray, y: np.ndarray):\n",
    "        delta_b = [np.zeros(b.shape) for b in self.all_bias]\n",
    "        delta_w = [np.zeros(w.shape) for w in self.all_weights]\n",
    "        activations, zs = self.prop_forward(x)\n",
    "        delta = activations[-1] - y #* softmax_prime(zs[-1]) #use zs delta\n",
    "        delta_b[-1] = delta\n",
    "        delta_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # loop through layers and now apply the sigmoid prime\n",
    "        for i in range(2, len(self.layers)):\n",
    "            sp = act_prime(zs[-i]) # No need to print here\n",
    "            delta = np.dot(self.all_weights[-i + 1].transpose(), delta) * sp \n",
    "            delta_b[-i] = delta\n",
    "            delta_w[-i] = np.dot(delta, activations[-i - 1].T)\n",
    "        return delta_w, delta_b\"\"\"\n",
    "    def prop_backward(self, x_batch: np.ndarray, y_batch: np.ndarray):\n",
    "        # Initialize gradients as zero\n",
    "        delta_b_sum = [np.zeros(b.shape) for b in self.all_bias]\n",
    "        delta_w_sum = [np.zeros(w.shape) for w in self.all_weights]\n",
    "\n",
    "        # Loop over each example in the batch\n",
    "        for x, y in zip(x_batch, y_batch):\n",
    "            activations, zs = self.prop_forward(x)\n",
    "            delta = activations[-1] - y.reshape(46,1)  # Assuming cost_derivative is defined elsewhere\n",
    "\n",
    "            # Gradients for output layer\n",
    "            delta_b_sum[-1] += delta\n",
    "            delta_w_sum[-1] += np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "            # Gradients for hidden layers\n",
    "            for i in range(2, len(self.layers)):\n",
    "                z = zs[-i]\n",
    "                sp = act_prime(z)\n",
    "                delta = np.dot(self.all_weights[-i + 1].transpose(), delta) * sp\n",
    "                delta_b_sum[-i] += delta\n",
    "                delta_w_sum[-i] += np.dot(delta, activations[-i - 1].T)\n",
    "\n",
    "        # Average the gradients over the mini-batch\n",
    "        num_examples = len(x_batch)\n",
    "        delta_b_avg = [db_sum / num_examples for db_sum in delta_b_sum]\n",
    "        delta_w_avg = [dw_sum / num_examples for dw_sum in delta_w_sum]\n",
    "\n",
    "        # Return the average gradients\n",
    "        return delta_w_avg, delta_b_avg\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, learning_rate):\n",
    "        # aggregate gradients from the mini_batch\n",
    "        delta_w, delta_b = self.prop_backward(mini_batch[0], mini_batch[1])\n",
    "    \n",
    "        # update weights and biases\n",
    "        self.all_weights = [w - (learning_rate * dw) for w, dw in zip(self.all_weights, delta_w)]\n",
    "        self.all_bias = [b - (learning_rate * db) for b, db in zip(self.all_bias, delta_b)]\n",
    "    \n",
    "\n",
    "    def train(self, X_train, y_train, epochs, learning_rate, mini_batch_size):\n",
    "        n = len(X_train)\n",
    "\n",
    "        # Convert to NumPy arrays if necessary\n",
    "        X_train_np = np.array(X_train) if not isinstance(X_train, np.ndarray) else X_train\n",
    "        y_train_np = np.array(y_train) if not isinstance(y_train, np.ndarray) else y_train\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data for each epoch\n",
    "            permutation = np.random.permutation(n)\n",
    "            X_train_shuffled = X_train_np[permutation]\n",
    "            y_train_shuffled = y_train_np[permutation]\n",
    "        \n",
    "            # Partition training data into mini-batches\n",
    "            mini_batches = [\n",
    "                (X_train_shuffled[k:k+mini_batch_size], y_train_shuffled[k:k+mini_batch_size])\n",
    "                for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "        \n",
    "            # Update the model's weights with each mini-batch\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, learning_rate)\n",
    "\n",
    "            new_loss = self.calc_squared_loss(X_train, y_train)\n",
    "            print(f\"Epoch {epoch}, mean squared loss: {new_loss}\")\n",
    "    \n",
    "    \n",
    "    \"\"\"def train(self, X_train, y_train, epochs=100, lr=1, verbose=True) -> None:\n",
    "            # Shuffle the data at the start of each epoch (optional)\n",
    "        e = 0\n",
    "        while (e < epochs):\n",
    "            rng = np.random.permutation(1024)\n",
    "            d_x = 1024\n",
    "            d_y = 46\n",
    "            for i in range(100):\n",
    "                x = X_train[rng[i]].reshape((d_x, 1))\n",
    "                y = y_train[rng[i]].reshape((d_y, 1))\n",
    "                delta_w, delta_b = self.prop_backward(x, y)\n",
    "                for i in range(len(self.all_weights)):\n",
    "                    self.all_weights[i] -= lr * delta_w[i]\n",
    "                    self.all_bias[i] -= lr * delta_b[i]\n",
    "\n",
    "            new_loss = self.calc_squared_loss(X_train, y_train)\n",
    "            if verbose:\n",
    "                print(f\"Epoch {e}, mean squared loss: {new_loss}\")\n",
    "            e+=1\"\"\"\n",
    "\n",
    "    def calc_squared_loss(self, X, y):\n",
    "        squared_errors = []\n",
    "        for i in range(len(X)):\n",
    "            y_pred, _ = self.prop_forward(X[i].reshape(1024, 1))  # prop_forward should give the prediction\n",
    "            squared_error = np.sum((y[i].reshape(46, 1) - y_pred[-1]) ** 2) / 2 \n",
    "            squared_errors.append(squared_error)\n",
    "        \n",
    "        # Calculate mean squared error over all examples.\n",
    "        mse = np.mean(squared_errors)\n",
    "        return mse\n",
    "\n",
    "    def pred(self, img: np.array):\n",
    "        img, _ = self.prop_forward(img)\n",
    "        return list(img[-1])\n",
    "    \n",
    "    def vals_for_softmax(self, index: int) -> np.array:\n",
    "        \"\"\" Helper function to create softmax values\"\"\"\n",
    "        vals = np.zeros(46)\n",
    "        vals[index] = 1\n",
    "        vals.reshape(46, 1)\n",
    "        return vals\n",
    "    \n",
    "    def validate(self, test_df: pd.DataFrame) -> None:\n",
    "        count, countright = 0, 0\n",
    "        for img_dir, row in test_df.iterrows():\n",
    "            img = np.array(Image.open(img_dir).convert(\"L\").resize((32, 32))).flatten().reshape(1024, 1)\n",
    "            outputs = list(self.pred(img))\n",
    "            actual = list(self.vals_for_softmax(row[0]))\n",
    "            if (list(self.pred(outputs)).index(list(max(self.pred(outputs)))) - list(actual).index(list(max(actual))) < 2):\n",
    "                countright += 1\n",
    "            count += 1\n",
    "            accuracy = (float(countright) / float(count)) * 100\n",
    "        print(\"count: \", count, \"countright: \", countright, \". Accuracy: \", accuracy, \"%\")\n",
    "\n",
    "    def save(self) -> None:\n",
    "        hp_data = '/Users/advaysingh/Documents/projects/hindi_classification/data/hyper_p'\n",
    "        hp_data_biases, hp_data_weights = hp_data + '/Biases', hp_data + '/Weights'\n",
    "        os.makedirs(hp_data)\n",
    "        os.makedirs(hp_data_biases)\n",
    "        os.makedirs(hp_data_weights)\n",
    "        for layer in range(len(self.layers)):\n",
    "            df_weights = pd.DataFrame(self.all_weights[layer])\n",
    "            df_bias = pd.DataFrame(self.all_bias[layer])\n",
    "            weights_file_name = \"layer_\" + str(layer) + \"weights.csv\"\n",
    "            bias_file_name = \"layer_\" + str(layer) + \"bias.csv\"\n",
    "            weights_file = open(os.path.join(hp_data_weights, weights_file_name), \"x\")\n",
    "            bias_file = open(os.path.join(hp_data_biases, bias_file_name), \"x\")\n",
    "            df_weights.to_csv(weights_file, index=False)\n",
    "            df_bias.to_csv(bias_file, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vals_for_softmax(index: int) -> np.array:\n",
    "        \"\"\" Helper function to create softmax values\"\"\"\n",
    "        vals = np.zeros(46)\n",
    "        vals[index] = 1\n",
    "        vals.reshape(46, 1)\n",
    "        return vals\n",
    "test_df_sample = test_df.sample(500)\n",
    "\n",
    "\n",
    "X_train, y_train = [], []\n",
    "for img_dir, row in train_df.iterrows():\n",
    "            X_train.append(np.array(Image.open(img_dir)).flatten().reshape(1024, 1))\n",
    "            y_train.append(vals_for_softmax(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, mean squared loss: 0.4202735903651491\n",
      "Epoch 1, mean squared loss: 0.3311629616681811\n",
      "Epoch 2, mean squared loss: 0.2939424643262792\n",
      "Epoch 3, mean squared loss: 0.27262557224676176\n",
      "Epoch 4, mean squared loss: 0.2578985695381331\n",
      "Epoch 5, mean squared loss: 0.24976406278643373\n",
      "Epoch 6, mean squared loss: 0.24045950648587333\n",
      "Epoch 7, mean squared loss: 0.23281125685917167\n",
      "Epoch 8, mean squared loss: 0.22752016746863832\n",
      "Epoch 9, mean squared loss: 0.22018771887618302\n",
      "Epoch 10, mean squared loss: 0.2148399322830054\n",
      "Epoch 11, mean squared loss: 0.2091867436543063\n",
      "Epoch 12, mean squared loss: 0.20674622459243705\n",
      "Epoch 13, mean squared loss: 0.2008296856864922\n",
      "Epoch 14, mean squared loss: 0.19567279901059612\n",
      "Epoch 15, mean squared loss: 0.19092184426832273\n",
      "Epoch 16, mean squared loss: 0.18830549743266348\n",
      "Epoch 17, mean squared loss: 0.18356197468846489\n",
      "Epoch 18, mean squared loss: 0.17859175265429153\n",
      "Epoch 19, mean squared loss: 0.17714996533554667\n",
      "Epoch 20, mean squared loss: 0.17318573981355956\n",
      "Epoch 21, mean squared loss: 0.17132432935989106\n",
      "Epoch 22, mean squared loss: 0.16584558786248982\n",
      "Epoch 23, mean squared loss: 0.16550294497925702\n",
      "Epoch 24, mean squared loss: 0.16079541260142757\n",
      "Epoch 25, mean squared loss: 0.16022053050144283\n",
      "Epoch 26, mean squared loss: 0.15519108435739362\n",
      "Epoch 27, mean squared loss: 0.15433133261901436\n",
      "Epoch 28, mean squared loss: 0.15290071051180656\n",
      "Epoch 29, mean squared loss: 0.15213355523762812\n",
      "Epoch 30, mean squared loss: 0.14706613388141185\n",
      "Epoch 31, mean squared loss: 0.14717290143316275\n",
      "Epoch 32, mean squared loss: 0.14496145660318976\n",
      "Epoch 33, mean squared loss: 0.14317127280004968\n",
      "Epoch 34, mean squared loss: 0.14085095346148277\n",
      "Epoch 35, mean squared loss: 0.13843588720300568\n",
      "Epoch 36, mean squared loss: 0.13789533690710876\n",
      "Epoch 37, mean squared loss: 0.137131385170803\n",
      "Epoch 38, mean squared loss: 0.13375891138974969\n",
      "Epoch 39, mean squared loss: 0.13290116102805544\n",
      "Epoch 40, mean squared loss: 0.1322105698771694\n",
      "Epoch 41, mean squared loss: 0.13177386377246783\n",
      "Epoch 42, mean squared loss: 0.13060461485167701\n",
      "Epoch 43, mean squared loss: 0.1292856606089052\n",
      "Epoch 44, mean squared loss: 0.12837578239349176\n",
      "Epoch 45, mean squared loss: 0.1274438807794177\n",
      "Epoch 46, mean squared loss: 0.12719366561700446\n",
      "Epoch 47, mean squared loss: 0.12403763286870681\n",
      "Epoch 48, mean squared loss: 0.12311592169913668\n",
      "Epoch 49, mean squared loss: 0.12340160604815865\n",
      "Epoch 50, mean squared loss: 0.12008703436884284\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[937], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m cnn \u001b[38;5;241m=\u001b[39m Model([\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m46\u001b[39m], \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[931], line 149\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, X_train, y_train, epochs, learning_rate, mini_batch_size)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Update the model's weights with each mini-batch\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mini_batch \u001b[38;5;129;01min\u001b[39;00m mini_batches:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m new_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_squared_loss(X_train, y_train)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, mean squared loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[931], line 121\u001b[0m, in \u001b[0;36mModel.update_mini_batch\u001b[0;34m(self, mini_batch, learning_rate)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_mini_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, mini_batch, learning_rate):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# aggregate gradients from the mini_batch\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     delta_w, delta_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprop_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# update weights and biases\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_weights \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;241m-\u001b[39m (learning_rate \u001b[38;5;241m*\u001b[39m dw) \u001b[38;5;28;01mfor\u001b[39;00m w, dw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_weights, delta_w)]\n",
      "Cell \u001b[0;32mIn[931], line 96\u001b[0m, in \u001b[0;36mModel.prop_backward\u001b[0;34m(self, x_batch, y_batch)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Loop over each example in the batch\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x_batch, y_batch):\n\u001b[0;32m---> 96\u001b[0m     activations, zs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprop_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     delta \u001b[38;5;241m=\u001b[39m activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m46\u001b[39m,\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assuming cost_derivative is defined elsewhere\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Gradients for output layer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[931], line 64\u001b[0m, in \u001b[0;36mModel.prop_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m zs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[0;32m---> 64\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_bias[i]\n\u001b[1;32m     65\u001b[0m     zs\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "cnn = Model([300, 100, 46], False)\n",
    "\n",
    "cnn.train(X_train, y_train, 5000, 0.5, mini_batch_size=int(300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 46 is different from 1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[929], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cnn \u001b[38;5;241m=\u001b[39m Model([\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m46\u001b[39m], \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[928], line 203\u001b[0m, in \u001b[0;36mModel.validate\u001b[0;34m(self, test_df)\u001b[0m\n\u001b[1;32m    201\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred(img))\n\u001b[1;32m    202\u001b[0m actual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvals_for_softmax(row[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred(outputs)))) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlist\u001b[39m(actual)\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmax\u001b[39m(actual))) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    204\u001b[0m     countright \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    205\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[928], line 187\u001b[0m, in \u001b[0;36mModel.pred\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpred\u001b[39m(\u001b[38;5;28mself\u001b[39m, img: np\u001b[38;5;241m.\u001b[39marray):\n\u001b[0;32m--> 187\u001b[0m     img, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprop_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(img[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[928], line 64\u001b[0m, in \u001b[0;36mModel.prop_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m zs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[0;32m---> 64\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_bias[i]\n\u001b[1;32m     65\u001b[0m     zs\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 46 is different from 1024)"
     ]
    }
   ],
   "source": [
    "cnn = Model([10, 46], True)\n",
    "cnn.validate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update stopped by user.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "cnn = Model([100, 50, 46], True)\n",
    "\"\"\"outs = cnn.pred(cnn.img_to_np(img_path))\n",
    "index = outs.index(max(outs))\n",
    "new_english_value = pd.read_csv(os.path.join(workspace, 'data', 'dict.csv'))[index][1]\"\"\"\n",
    "\n",
    "def update_json_file(hindi_value, english_value):\n",
    "    with open(out_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    data['hindi'] = hindi_value\n",
    "    data['english'] = english_value\n",
    "    with open(out_file, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# Define a list of possible values if applicable\n",
    "hindi_values = [\"हिं\", \"नया\", \"मूल्य\", \"उदाहरण\"]\n",
    "english_values = [\"Eng\", \"New\", \"Value\", \"Example\"]\n",
    "\n",
    "# Loop to update the JSON file every second with a random value\n",
    "try:\n",
    "    while True:\n",
    "        # Generate a random value from the lists\n",
    "        new_hindi_value = random.choice(hindi_values)\n",
    "        #new_english_value = random.choice(english_values)\n",
    "        img = np.array(Image.open(img_path).convert(\"L\").resize((32, 32))).flatten().reshape(1024, 1)\n",
    "        index = list(cnn.pred(img)).index(list(max(cnn.pred(img)))) #.index(max(cnn.pred(img)))\n",
    "        new_english_value = str(pd.read_csv(os.path.join(workspace, 'data', 'dict.csv')).iloc[index -1, 1])\n",
    "\n",
    "        update_json_file(new_hindi_value, new_english_value)\n",
    "        \n",
    "        # Print the new values (for debug purposes)\n",
    "        # print(f'Updated JSON file with hindi: {new_hindi_value}, english: {new_english_value}')\n",
    "\n",
    "        # read in and resize img\n",
    " \n",
    "        # read in image for debugging\n",
    "        \"\"\"img.show(\"img to classify\")\n",
    "        print(img.format)\n",
    "        print(img.mode)\n",
    "        print(img.size)\"\"\"\n",
    "\n",
    "        # create np array and resize\n",
    "        \"\"\"img_data = np.array(img).flatten()\n",
    "        print(img_data.shape)\n",
    "        print(img_data)\"\"\"\n",
    "\n",
    "        \n",
    "        time.sleep(1)  # Wait for 1 second\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Update stopped by user.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
